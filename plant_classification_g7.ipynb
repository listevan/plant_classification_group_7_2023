{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecbe5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch and numpy and pretrained model\n",
    "from torchvision import models\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# load pretrained model\n",
    "pretrained_model = models.resnet50(weights = 'DEFAULT', progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262de170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes fully connected/classifier layer to new layer for us to train, 15 is the number of solutions or types of leaves\n",
    "pretrained_model.fc = torch.nn.Linear(2048, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a04fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Currently not in use, for if we wanna custom build a softmax but idk how to train it\n",
    "# import torch.nn as nn\n",
    "# # build custom softmax module\n",
    "# class Softmax(nn.Module):\n",
    "#     def __init__(self, n_inputs, n_outputs):\n",
    "#         super().__init__()\n",
    "#         self.linear = nn.Linear(n_inputs, n_outputs)\n",
    " \n",
    "#     def forward(self, x):\n",
    "#         pred = self.linear(x)\n",
    "#         return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d631a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Currently not in use\n",
    "# # adds softmax to model\n",
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, pretrained_model):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.pretrained_model = pretrained_model\n",
    "#         self.last_layer = Softmax(1000, n) # add how many nodes as input and output\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.last_layer(self.pretrained_model(x))\n",
    "\n",
    "# model = MyModel(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c430901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze model except fc layer because we don't wanna retrain the pretrained model\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in pretrained_model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e32582",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss() #could write this out ourselves\n",
    "# need to find an optimizer or make one for a custom softmax function\n",
    "optimizer = torch.optim.Adam(pretrained_model.fc.parameters(), lr=0.001) #need optimize learning rate idk how momentum works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3530c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenli/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "data_dir = \"/Users/stevenli/SigAida/data/images/\" #change to data directory\n",
    "\n",
    "# gets the label based on the number\n",
    "def getLabel(s):\n",
    "    labels = {1 : 'Ulmus carpinifolia', \n",
    "                2 : 'Acer', \n",
    "                3 : 'Salix aurita', \n",
    "                4 : 'Quercus', \n",
    "                5 : 'Alnus incan', \n",
    "                6 : 'Betula pubescens', \n",
    "                7 : 'Salix alba \\'Sericea\\'', \n",
    "                8 : 'Populus tremula', \n",
    "                9 : 'Ulmus glabra', \n",
    "                10 : 'Sorbus aucuparia', \n",
    "                11 : 'Salix sinerea', \n",
    "                12 : 'Populus', \n",
    "                13 : 'Tilia', \n",
    "                14 : 'Sorbus intermedia', \n",
    "                15 : 'Fagus silvatica'}\n",
    "    return labels.get(int(s))\n",
    "\n",
    "im = [] # images\n",
    "la = [] # labels not in use, because you can't feed strings to gpu, need to feed tensors\n",
    "ohe = [] #s toring int values for one hot encodings\n",
    "joined = []\n",
    "\n",
    "for f in os.listdir(data_dir):\n",
    "    im.append(data_dir + f)\n",
    "    val = int(f[:-9].replace('l','')) # removes last 9 letters replaces l with blank then gets label\n",
    "    la.append(getLabel(val)) \n",
    "    ohe.append(val)\n",
    "\n",
    "#one hot encoding (changing 1-15 to tensors for gpu)\n",
    "encoded = OneHotEncoder(categories = [[x for x in range(1, 16)]], sparse = False).fit_transform(np.array(ohe).reshape((len(ohe),1)))\n",
    "# print(encoded)\n",
    "\n",
    "#joining image and label for organization and if we wanna shuffle\n",
    "for image, label in zip(im, encoded):\n",
    "    joined.append([image, label])\n",
    "\n",
    "# print(la)\n",
    "# print(im)\n",
    "# print(joined[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e2c21ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125\n",
      "900\n",
      "112\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "# some preprocessing, randomize data, split data, load into dataloaders\n",
    "import random\n",
    "\n",
    "# splits (train, val, test) test currently not in use\n",
    "split_ratio=(0.8, 0.1, 0.1)\n",
    "\n",
    "def create_splits(data, split_ratio):\n",
    "    random.shuffle(data)\n",
    "    train = [data[i] for i in range(0, round(.8 * len(data)))]\n",
    "    val = [data[i] for i in range(round(.8 * len(data)), round(.9 * len(data)))]\n",
    "    test = [data[i] for i in range(round(.9 * len(data)), len(data))]\n",
    "    return train, val, test\n",
    "\n",
    "train, val, test = create_splits(joined, split_ratio)\n",
    "\n",
    "print(len(joined))\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f5b89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# makes a custom dataset based on pytorch dataset class\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, data_dir, arr, transform = None):\n",
    "        # initialize some valuess\n",
    "        self.data_dir = data_dir\n",
    "        self.data = [x[0] for x in arr]\n",
    "        self.arr = arr\n",
    "\n",
    "        #transform to normalize/resize all images\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.arr)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.transform(Image.open(self.arr[idx][0]))\n",
    "        s = self.arr[idx][1]\n",
    "        #returns a tuple of the transformed image and the label (one-hot encoding)\n",
    "        return (img, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7556e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "#defining our transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224), # change to what data should be\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "#making dataset and dataloader (gives data to model, using batch_size 1 cuz google says that's good for sgd (stochastic gradient descent))\n",
    "train_dataset = PlantDataset(data_dir, train, transform = transform) #probably need to make this bigger\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8160eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#chcek devices change to whatever you're using\n",
    "\n",
    "#mps = (m1/m2 mac) gpu\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "#set device\n",
    "device = torch.device('mps')\n",
    "\n",
    "from tqdm import tqdm # progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35efd478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [05:05<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basline before training\n",
      "0.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#testing accuracy on test dataset\n",
    "total_correct = 0\n",
    "total_instances = 0\n",
    "\n",
    "pretrained_model.eval()\n",
    "# iterating through batches without updating gradients\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(train_loader):\n",
    "      pretrained_model.to(device)\n",
    "      images = images.to(device)\n",
    "      # labels = labels.float().to(device) # don't need this cuz we not training no more\n",
    "\n",
    "      # making classifications and deriving indices of maximum value via argmax (which gives the max value i the tensor)\n",
    "      solution_tensor = pretrained_model(images)\n",
    "      classifications = torch.argmax(solution_tensor, dim = 1).item()\n",
    "\n",
    "      #undoing one-hot encoding to get label value as a number\n",
    "      label = np.where(labels.numpy() == 1)[1]\n",
    "\n",
    "      correct_predictions = int(classifications==label)\n",
    "\n",
    "      #  incrementing counters\n",
    "      total_correct+=correct_predictions\n",
    "      total_instances+=len(images)\n",
    "\n",
    "#print accuracy\n",
    "print(\"basline before training\")\n",
    "print(round(total_correct/total_instances, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "971f44ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 377, 224] at entry 0 and [3, 364, 224] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/stevenli/SigAida/plant_classification_group_7_2023/plant_classification_g7.ipynb Cell 13\u001b[0m line \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stevenli/SigAida/plant_classification_group_7_2023/plant_classification_g7.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stevenli/SigAida/plant_classification_group_7_2023/plant_classification_g7.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stevenli/SigAida/plant_classification_group_7_2023/plant_classification_g7.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mfor\u001b[39;00m data, label \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stevenli/SigAida/plant_classification_group_7_2023/plant_classification_g7.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m# Move tensors to the configured device\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stevenli/SigAida/plant_classification_group_7_2023/plant_classification_g7.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         pretrained_model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stevenli/SigAida/plant_classification_group_7_2023/plant_classification_g7.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         images \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 377, 224] at entry 0 and [3, 364, 224] at entry 1"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "pretrained_model.train()\n",
    "num_epochs = 200\n",
    "\n",
    "#store loss\n",
    "loss_history = []\n",
    "\n",
    "#change batch size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data, label in tqdm(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        pretrained_model.to(device)\n",
    "        images = data.to(device)\n",
    "        labels = label.float().to(device) #.float() for some float64, float32 conversion thing, don't completely understand\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = pretrained_model(images) #currently image tensor don't match fc layer if you use non pretrained model\n",
    "        loss = criterion(outputs, labels) #calculate loss\n",
    "\n",
    "        # Backward and optimize don't really understand this stuff\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print some statistics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    loss_history.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270f96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/900 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:51<00:00, 17.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training\n",
      "0.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#testing accuracy on test dataset\n",
    "total_correct = 0\n",
    "total_instances = 0\n",
    "\n",
    "#chang back batch size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "pretrained_model.eval()\n",
    "# iterating through batches without updating gradients\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(train_loader):\n",
    "      images = images.to(device)\n",
    "      # labels = labels.float().to(device) # don't need this cuz we not training no more\n",
    "\n",
    "      # making classifications and deriving indices of maximum value via argmax (which gives the max value i the tensor)\n",
    "      solution_tensor = pretrained_model(images)\n",
    "      classifications = torch.argmax(solution_tensor, dim = 1).item()\n",
    "\n",
    "      #undoing one-hot encoding to get label value as a number\n",
    "      label = np.where(labels.numpy() == 1)[1]\n",
    "\n",
    "      correct_predictions = int(classifications==label)\n",
    "\n",
    "      #  incrementing counters\n",
    "      total_correct+=correct_predictions\n",
    "      total_instances+=len(images)\n",
    "\n",
    "#print accuracy\n",
    "print(\"after training\")\n",
    "print(round(total_correct/total_instances, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99bea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading validation dataset\n",
    "val_dataset = PlantDataset(data_dir, val, transform = transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9f411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:06<00:00, 16.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_instances = 0\n",
    "\n",
    "pretrained_model.eval()\n",
    "# iterating through batches without updating gradients\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader):\n",
    "      images = images.to(device)\n",
    "      # labels = labels.float().to(device) # don't need this cuz we not training no more\n",
    "\n",
    "      # making classifications and deriving indices of maximum value via argmax (which gives the max value i the tensor)\n",
    "      solution_tensor = pretrained_model(images)\n",
    "      classifications = torch.argmax(solution_tensor, dim = 1).item()\n",
    "\n",
    "      #undoing one-hot encoding to get label value as a number\n",
    "      label = np.where(labels.numpy() == 1)[1]\n",
    "\n",
    "      correct_predictions = int(classifications==label)\n",
    "\n",
    "      #  incrementing counters\n",
    "      total_correct+=correct_predictions\n",
    "      total_instances+=len(images)\n",
    "\n",
    "#print accuracy\n",
    "print(round(total_correct/total_instances, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading test dataset\n",
    "test_dataset = PlantDataset(data_dir, test, transform = transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d11ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:06<00:00, 17.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_instances = 0\n",
    "\n",
    "pictures = []\n",
    "model_guesses = []\n",
    "correct_answers = []\n",
    "\n",
    "# iterating through batches without updating gradients\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "      pictures.append(images)\n",
    "      images = images.to(device)\n",
    "\n",
    "      # making classifications and deriving indices of maximum value via argmax (which gives the max value i the tensor)\n",
    "      solution_tensor = pretrained_model(images)\n",
    "      classifications = torch.argmax(solution_tensor, dim = 1).item()\n",
    "      model_guesses.append(getLabel(classifications + 1)) # + 1 to convert from index to dict key\n",
    "\n",
    "      #undoing one-hot encoding to get label value as a number\n",
    "      label = np.where(labels.numpy() == 1)[1]\n",
    "      correct_answers.append(getLabel(label + 1)) # + 1 to convert from index to dict key\n",
    "\n",
    "      correct_predictions = int(classifications==label)\n",
    "\n",
    "      #  incrementing counters\n",
    "      total_correct+=correct_predictions\n",
    "      total_instances+=len(images)\n",
    "\n",
    "#print accuracy\n",
    "print(round(total_correct/total_instances, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b79b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our classifiction is:  Betula pubescens  and the correct classification is  Betula pubescens\n",
      "Our classifiction is:  Ulmus glabra  and the correct classification is  Salix aurita\n",
      "Our classifiction is:  Quercus  and the correct classification is  Quercus\n"
     ]
    }
   ],
   "source": [
    "test_indices = [random.randint(0, len(pictures)) for x in range(3)] # 3 is arbitray choose as many as you wanna show\n",
    "transform = transforms.ToPILImage()\n",
    "for test in test_indices:\n",
    "    img = transform(torch.reshape(pictures[test], (3, 224, 224)))\n",
    "    img.show()\n",
    "    print('Our classifiction is: ', model_guesses[test], ' and the correct classification is ', correct_answers[test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
