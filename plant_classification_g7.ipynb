{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecbe5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch and numpy and pretrained model\n",
    "from torchvision import models\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# load pretrained model\n",
    "pretrained_model = models.resnet50(weights = 'DEFAULT', progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262de170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes fully connected/classifier layer to new layer for us to train, 15 is the number of solutions or types of leaves\n",
    "pretrained_model.fc = torch.nn.Linear(2048, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a04fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Currently not in use, for if we wanna custom build a softmax but idk how to train it\n",
    "# import torch.nn as nn\n",
    "# # build custom softmax module\n",
    "# class Softmax(nn.Module):\n",
    "#     def __init__(self, n_inputs, n_outputs):\n",
    "#         super().__init__()\n",
    "#         self.linear = nn.Linear(n_inputs, n_outputs)\n",
    " \n",
    "#     def forward(self, x):\n",
    "#         pred = self.linear(x)\n",
    "#         return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d631a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Currently not in use\n",
    "# # adds softmax to model\n",
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, pretrained_model):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.pretrained_model = pretrained_model\n",
    "#         self.last_layer = Softmax(1000, n) # add how many nodes as input and output\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.last_layer(self.pretrained_model(x))\n",
    "\n",
    "# model = MyModel(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c430901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze model except fc layer because we don't wanna retrain the pretrained model\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in pretrained_model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e32582",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss() #could write this out ourselves\n",
    "# need to find an optimizer or make one for a custom softmax function\n",
    "optimizer = torch.optim.Adam(pretrained_model.fc.parameters(), lr=0.001) #need optimize learning rate idk how momentum works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3530c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l8nr039.tif\n",
      "l9nr039.tif\n",
      "l9nr011.tif\n",
      "l8nr005.tif\n",
      "l8nr011.tif\n",
      "l9nr005.tif\n",
      "l10nr050.tif\n",
      "l11nr044.tif\n",
      "l11nr050.tif\n",
      "l10nr044.tif\n",
      "l3nr006.tif\n",
      "l2nr012.tif\n",
      "l2nr006.tif\n",
      "l3nr012.tif\n",
      "l1nr055.tif\n",
      "l1nr041.tif\n",
      "l12nr003.tif\n",
      "l1nr069.tif\n",
      "l13nr017.tif\n",
      "l13nr003.tif\n",
      "l12nr017.tif\n",
      "l5nr063.tif\n",
      "l4nr063.tif\n",
      "l6nr018.tif\n",
      "l15nr072.tif\n",
      "l14nr066.tif\n",
      "l7nr018.tif\n",
      "l14nr072.tif\n",
      "l15nr066.tif\n",
      "l6nr024.tif\n",
      "l7nr030.tif\n",
      "l7nr024.tif\n",
      "l6nr030.tif\n",
      "l7nr025.tif\n",
      "l6nr031.tif\n",
      "l6nr025.tif\n",
      "l7nr031.tif\n",
      "l14nr073.tif\n",
      "l7nr019.tif\n",
      "l15nr067.tif\n",
      "l15nr073.tif\n",
      "l6nr019.tif\n",
      "l14nr067.tif\n",
      "l4nr062.tif\n",
      "l5nr062.tif\n",
      "l13nr002.tif\n",
      "l12nr016.tif\n",
      "l1nr068.tif\n",
      "l12nr002.tif\n",
      "l13nr016.tif\n",
      "l1nr040.tif\n",
      "l1nr054.tif\n",
      "l2nr007.tif\n",
      "l3nr013.tif\n",
      "l3nr007.tif\n",
      "l2nr013.tif\n",
      "l11nr051.tif\n",
      "l10nr045.tif\n",
      "l10nr051.tif\n",
      "l11nr045.tif\n",
      "l8nr010.tif\n",
      "l9nr004.tif\n",
      "l9nr010.tif\n",
      "l8nr004.tif\n",
      "l9nr038.tif\n",
      "l8nr038.tif\n",
      "l9nr006.tif\n",
      "l8nr012.tif\n",
      "l8nr006.tif\n",
      "l9nr012.tif\n",
      "l10nr047.tif\n",
      "l11nr053.tif\n",
      "l2nr039.tif\n",
      "l11nr047.tif\n",
      "l10nr053.tif\n",
      "l3nr039.tif\n",
      "l3nr011.tif\n",
      "l2nr005.tif\n",
      "l2nr011.tif\n",
      "l3nr005.tif\n",
      "l12nr028.tif\n",
      "l1nr042.tif\n",
      "l13nr028.tif\n",
      "l1nr056.tif\n",
      "l12nr014.tif\n",
      "l13nr014.tif\n",
      "l4nr060.tif\n",
      "l5nr074.tif\n",
      "l5nr060.tif\n",
      "l4nr074.tif\n",
      "l5nr048.tif\n",
      "l4nr048.tif\n",
      "l15nr065.tif\n",
      "l14nr071.tif\n",
      "l14nr065.tif\n",
      "l15nr071.tif\n",
      "l6nr033.tif\n",
      "l15nr059.tif\n",
      "l7nr027.tif\n",
      "l7nr033.tif\n",
      "l14nr059.tif\n",
      "l6nr027.tif\n",
      "l14nr058.tif\n",
      "l7nr032.tif\n",
      "l6nr026.tif\n",
      "l15nr058.tif\n",
      "l6nr032.tif\n",
      "l7nr026.tif\n",
      "l14nr064.tif\n",
      "l15nr070.tif\n",
      "l15nr064.tif\n",
      "l14nr070.tif\n",
      "l4nr049.tif\n",
      "l5nr049.tif\n",
      "l5nr061.tif\n",
      "l4nr075.tif\n",
      "l4nr061.tif\n",
      "l5nr075.tif\n",
      "l13nr015.tif\n",
      "l12nr001.tif\n",
      "l12nr015.tif\n",
      "l13nr001.tif\n",
      "l13nr029.tif\n",
      "l1nr057.tif\n",
      "l1nr043.tif\n",
      "l12nr029.tif\n",
      "l2nr010.tif\n",
      "l3nr004.tif\n",
      "l3nr010.tif\n",
      "l2nr004.tif\n",
      "l11nr046.tif\n",
      "l3nr038.tif\n",
      "l10nr052.tif\n",
      "l10nr046.tif\n",
      "l2nr038.tif\n",
      "l11nr052.tif\n",
      "l8nr007.tif\n",
      "l9nr013.tif\n",
      "l9nr007.tif\n",
      "l8nr013.tif\n",
      "l8nr017.tif\n",
      "l9nr003.tif\n",
      "l9nr017.tif\n",
      "l8nr003.tif\n",
      "l3nr014.tif\n",
      "l2nr014.tif\n",
      "l11nr056.tif\n",
      "l10nr042.tif\n",
      "l3nr028.tif\n",
      "l10nr056.tif\n",
      "l11nr042.tif\n",
      "l2nr028.tif\n",
      "l13nr005.tif\n",
      "l12nr011.tif\n",
      "l12nr005.tif\n",
      "l13nr011.tif\n",
      "l13nr039.tif\n",
      "l1nr047.tif\n",
      "l12nr039.tif\n",
      "l1nr053.tif\n",
      "l4nr059.tif\n",
      "l5nr059.tif\n",
      "l5nr071.tif\n",
      "l4nr065.tif\n",
      "l4nr071.tif\n",
      "l5nr065.tif\n",
      "l7nr022.tif\n",
      "l14nr048.tif\n",
      "l6nr036.tif\n",
      "l6nr022.tif\n",
      "l15nr048.tif\n",
      "l7nr036.tif\n",
      "l14nr074.tif\n",
      "l15nr060.tif\n",
      "l15nr074.tif\n",
      "l14nr060.tif\n",
      "l15nr075.tif\n",
      "l14nr061.tif\n",
      "l14nr075.tif\n",
      "l15nr061.tif\n",
      "l15nr049.tif\n",
      "l6nr023.tif\n",
      "l7nr037.tif\n",
      "l14nr049.tif\n",
      "l7nr023.tif\n",
      "l6nr037.tif\n",
      "l4nr070.tif\n",
      "l5nr064.tif\n",
      "l5nr070.tif\n",
      "l4nr064.tif\n",
      "l5nr058.tif\n",
      "l4nr058.tif\n",
      "l1nr052.tif\n",
      "l12nr038.tif\n",
      "l13nr038.tif\n",
      "l1nr046.tif\n",
      "l12nr004.tif\n",
      "l13nr010.tif\n",
      "l13nr004.tif\n",
      "l12nr010.tif\n",
      "l10nr057.tif\n",
      "l2nr029.tif\n",
      "l11nr043.tif\n",
      "l11nr057.tif\n",
      "l3nr029.tif\n",
      "l10nr043.tif\n",
      "l3nr001.tif\n",
      "l2nr015.tif\n",
      "l2nr001.tif\n",
      "l3nr015.tif\n",
      "l9nr016.tif\n",
      "l8nr002.tif\n",
      "l8nr016.tif\n",
      "l9nr002.tif\n",
      "l9nr014.tif\n",
      "l8nr014.tif\n",
      "l9nr028.tif\n",
      "l8nr028.tif\n",
      "l2nr017.tif\n",
      "l10nr069.tif\n",
      "l3nr003.tif\n",
      "l3nr017.tif\n",
      "l11nr069.tif\n",
      "l2nr003.tif\n",
      "l11nr041.tif\n",
      "l10nr055.tif\n",
      "l10nr041.tif\n",
      "l11nr055.tif\n",
      "l13nr012.tif\n",
      "l12nr006.tif\n",
      "l12nr012.tif\n",
      "l13nr006.tif\n",
      "l1nr050.tif\n",
      "l1nr044.tif\n",
      "l5nr066.tif\n",
      "l4nr072.tif\n",
      "l4nr066.tif\n",
      "l5nr072.tif\n",
      "l7nr035.tif\n",
      "l6nr021.tif\n",
      "l6nr035.tif\n",
      "l7nr021.tif\n",
      "l7nr009.tif\n",
      "l14nr063.tif\n",
      "l6nr009.tif\n",
      "l15nr063.tif\n",
      "l15nr062.tif\n",
      "l6nr008.tif\n",
      "l14nr062.tif\n",
      "l7nr008.tif\n",
      "l6nr034.tif\n",
      "l7nr020.tif\n",
      "l7nr034.tif\n",
      "l6nr020.tif\n",
      "l4nr067.tif\n",
      "l5nr073.tif\n",
      "l5nr067.tif\n",
      "l4nr073.tif\n",
      "l1nr045.tif\n",
      "l1nr051.tif\n",
      "l12nr013.tif\n",
      "l13nr007.tif\n",
      "l13nr013.tif\n",
      "l12nr007.tif\n",
      "l10nr040.tif\n",
      "l11nr054.tif\n",
      "l11nr040.tif\n",
      "l10nr054.tif\n",
      "l3nr016.tif\n",
      "l2nr002.tif\n",
      "l11nr068.tif\n",
      "l2nr016.tif\n",
      "l3nr002.tif\n",
      "l10nr068.tif\n",
      "l8nr029.tif\n",
      "l9nr029.tif\n",
      "l9nr001.tif\n",
      "l8nr015.tif\n",
      "l8nr001.tif\n",
      "l9nr015.tif\n",
      "l8nr066.tif\n",
      "l9nr072.tif\n",
      "l9nr066.tif\n",
      "l8nr072.tif\n",
      "l11nr027.tif\n",
      "l10nr033.tif\n",
      "l3nr059.tif\n",
      "l10nr027.tif\n",
      "l11nr033.tif\n",
      "l2nr059.tif\n",
      "l2nr071.tif\n",
      "l3nr065.tif\n",
      "l3nr071.tif\n",
      "l2nr065.tif\n",
      "l13nr048.tif\n",
      "l1nr036.tif\n",
      "l12nr048.tif\n",
      "l1nr022.tif\n",
      "l13nr074.tif\n",
      "l12nr060.tif\n",
      "l12nr074.tif\n",
      "l13nr060.tif\n",
      "l4nr014.tif\n",
      "l5nr014.tif\n",
      "l4nr028.tif\n",
      "l5nr028.tif\n",
      "l14nr005.tif\n",
      "l15nr011.tif\n",
      "l15nr005.tif\n",
      "l14nr011.tif\n",
      "l7nr053.tif\n",
      "l14nr039.tif\n",
      "l6nr047.tif\n",
      "l6nr053.tif\n",
      "l15nr039.tif\n",
      "l7nr047.tif\n",
      "l15nr038.tif\n",
      "l6nr052.tif\n",
      "l7nr046.tif\n",
      "l14nr038.tif\n",
      "l7nr052.tif\n",
      "l6nr046.tif\n",
      "l15nr004.tif\n",
      "l14nr010.tif\n",
      "l14nr004.tif\n",
      "l15nr010.tif\n",
      "l5nr029.tif\n",
      "l4nr029.tif\n",
      "l4nr001.tif\n",
      "l5nr015.tif\n",
      "l5nr001.tif\n",
      "l4nr015.tif\n",
      "l12nr075.tif\n",
      "l13nr061.tif\n",
      "l13nr075.tif\n",
      "l12nr061.tif\n",
      "l1nr023.tif\n",
      "l12nr049.tif\n",
      "l13nr049.tif\n",
      "l1nr037.tif\n",
      "l3nr070.tif\n",
      "l2nr064.tif\n",
      "l2nr070.tif\n",
      "l3nr064.tif\n",
      "l10nr026.tif\n",
      "l2nr058.tif\n",
      "l11nr032.tif\n",
      "l11nr026.tif\n",
      "l3nr058.tif\n",
      "l10nr032.tif\n",
      "l9nr067.tif\n",
      "l8nr073.tif\n",
      "l8nr067.tif\n",
      "l9nr073.tif\n",
      "l9nr059.tif\n",
      "l8nr059.tif\n",
      "l8nr071.tif\n",
      "l9nr065.tif\n",
      "l9nr071.tif\n",
      "l8nr065.tif\n",
      "l11nr030.tif\n",
      "l10nr024.tif\n",
      "l10nr030.tif\n",
      "l11nr024.tif\n",
      "l2nr066.tif\n",
      "l10nr018.tif\n",
      "l3nr072.tif\n",
      "l3nr066.tif\n",
      "l11nr018.tif\n",
      "l2nr072.tif\n",
      "l1nr021.tif\n",
      "l1nr035.tif\n",
      "l13nr063.tif\n",
      "l12nr063.tif\n",
      "l1nr009.tif\n",
      "l5nr017.tif\n",
      "l4nr003.tif\n",
      "l4nr017.tif\n",
      "l5nr003.tif\n",
      "l14nr012.tif\n",
      "l15nr006.tif\n",
      "l15nr012.tif\n",
      "l14nr006.tif\n",
      "l7nr044.tif\n",
      "l6nr050.tif\n",
      "l6nr044.tif\n",
      "l7nr050.tif\n",
      "l6nr045.tif\n",
      "l7nr051.tif\n",
      "l7nr045.tif\n",
      "l6nr051.tif\n",
      "l15nr013.tif\n",
      "l14nr007.tif\n",
      "l14nr013.tif\n",
      "l15nr007.tif\n",
      "l4nr016.tif\n",
      "l5nr002.tif\n",
      "l5nr016.tif\n",
      "l4nr002.tif\n",
      "l1nr008.tif\n",
      "l12nr062.tif\n",
      "l13nr062.tif\n",
      "l1nr034.tif\n",
      "l1nr020.tif\n",
      "l3nr067.tif\n",
      "l2nr073.tif\n",
      "l11nr019.tif\n",
      "l2nr067.tif\n",
      "l3nr073.tif\n",
      "l10nr019.tif\n",
      "l10nr031.tif\n",
      "l11nr025.tif\n",
      "l11nr031.tif\n",
      "l10nr025.tif\n",
      "l9nr070.tif\n",
      "l8nr064.tif\n",
      "l8nr070.tif\n",
      "l9nr064.tif\n",
      "l8nr058.tif\n",
      "l9nr058.tif\n",
      "l9nr060.tif\n",
      "l8nr074.tif\n",
      "l8nr060.tif\n",
      "l9nr074.tif\n",
      "l8nr048.tif\n",
      "l9nr048.tif\n",
      "l11nr009.tif\n",
      "l2nr063.tif\n",
      "l10nr009.tif\n",
      "l3nr063.tif\n",
      "l10nr021.tif\n",
      "l11nr035.tif\n",
      "l11nr021.tif\n",
      "l10nr035.tif\n",
      "l12nr072.tif\n",
      "l1nr018.tif\n",
      "l13nr066.tif\n",
      "l13nr072.tif\n",
      "l12nr066.tif\n",
      "l1nr024.tif\n",
      "l1nr030.tif\n",
      "l4nr006.tif\n",
      "l5nr012.tif\n",
      "l5nr006.tif\n",
      "l4nr012.tif\n",
      "l6nr055.tif\n",
      "l7nr041.tif\n",
      "l7nr055.tif\n",
      "l6nr041.tif\n",
      "l6nr069.tif\n",
      "l15nr003.tif\n",
      "l14nr017.tif\n",
      "l7nr069.tif\n",
      "l14nr003.tif\n",
      "l15nr017.tif\n",
      "l14nr002.tif\n",
      "l7nr068.tif\n",
      "l15nr016.tif\n",
      "l15nr002.tif\n",
      "l6nr068.tif\n",
      "l14nr016.tif\n",
      "l7nr054.tif\n",
      "l6nr040.tif\n",
      "l6nr054.tif\n",
      "l7nr040.tif\n",
      "l5nr007.tif\n",
      "l4nr013.tif\n",
      "l4nr007.tif\n",
      "l5nr013.tif\n",
      "l1nr031.tif\n",
      "l1nr025.tif\n",
      "l13nr073.tif\n",
      "l12nr067.tif\n",
      "l1nr019.tif\n",
      "l12nr073.tif\n",
      "l13nr067.tif\n",
      "l11nr020.tif\n",
      "l10nr034.tif\n",
      "l10nr020.tif\n",
      "l11nr034.tif\n",
      "l3nr062.tif\n",
      "l10nr008.tif\n",
      "l2nr062.tif\n",
      "l11nr008.tif\n",
      "l9nr049.tif\n",
      "l8nr049.tif\n",
      "l8nr061.tif\n",
      "l9nr075.tif\n",
      "l9nr061.tif\n",
      "l8nr075.tif\n",
      "l8nr063.tif\n",
      "l9nr063.tif\n",
      "l3nr060.tif\n",
      "l2nr074.tif\n",
      "l2nr060.tif\n",
      "l3nr074.tif\n",
      "l10nr036.tif\n",
      "l11nr022.tif\n",
      "l2nr048.tif\n",
      "l11nr036.tif\n",
      "l10nr022.tif\n",
      "l3nr048.tif\n",
      "l12nr065.tif\n",
      "l13nr071.tif\n",
      "l13nr065.tif\n",
      "l12nr071.tif\n",
      "l12nr059.tif\n",
      "l1nr033.tif\n",
      "l13nr059.tif\n",
      "l1nr027.tif\n",
      "l5nr039.tif\n",
      "l4nr039.tif\n",
      "l4nr011.tif\n",
      "l5nr005.tif\n",
      "l5nr011.tif\n",
      "l4nr005.tif\n",
      "l6nr042.tif\n",
      "l15nr028.tif\n",
      "l7nr056.tif\n",
      "l7nr042.tif\n",
      "l14nr028.tif\n",
      "l6nr056.tif\n",
      "l15nr014.tif\n",
      "l14nr014.tif\n",
      "l14nr015.tif\n",
      "l15nr001.tif\n",
      "l15nr015.tif\n",
      "l14nr001.tif\n",
      "l14nr029.tif\n",
      "l7nr043.tif\n",
      "l6nr057.tif\n",
      "l15nr029.tif\n",
      "l6nr043.tif\n",
      "l7nr057.tif\n",
      "l5nr010.tif\n",
      "l4nr004.tif\n",
      "l4nr010.tif\n",
      "l5nr004.tif\n",
      "l4nr038.tif\n",
      "l5nr038.tif\n",
      "l13nr058.tif\n",
      "l1nr026.tif\n",
      "l1nr032.tif\n",
      "l12nr058.tif\n",
      "l13nr064.tif\n",
      "l12nr070.tif\n",
      "l12nr064.tif\n",
      "l13nr070.tif\n",
      "l11nr037.tif\n",
      "l3nr049.tif\n",
      "l10nr023.tif\n",
      "l10nr037.tif\n",
      "l2nr049.tif\n",
      "l11nr023.tif\n",
      "l2nr061.tif\n",
      "l3nr075.tif\n",
      "l3nr061.tif\n",
      "l2nr075.tif\n",
      "l9nr062.tif\n",
      "l8nr062.tif\n",
      "l8nr047.tif\n",
      "l9nr053.tif\n",
      "l9nr047.tif\n",
      "l8nr053.tif\n",
      "l11nr006.tif\n",
      "l10nr012.tif\n",
      "l10nr006.tif\n",
      "l11nr012.tif\n",
      "l2nr050.tif\n",
      "l3nr044.tif\n",
      "l3nr050.tif\n",
      "l2nr044.tif\n",
      "l13nr069.tif\n",
      "l1nr017.tif\n",
      "l12nr069.tif\n",
      "l1nr003.tif\n",
      "l13nr055.tif\n",
      "l12nr041.tif\n",
      "l12nr055.tif\n",
      "l13nr041.tif\n",
      "l5nr021.tif\n",
      "l4nr035.tif\n",
      "l4nr021.tif\n",
      "l5nr035.tif\n",
      "l4nr009.tif\n",
      "l5nr009.tif\n",
      "l14nr024.tif\n",
      "l15nr030.tif\n",
      "l15nr024.tif\n",
      "l14nr030.tif\n",
      "l7nr072.tif\n",
      "l14nr018.tif\n",
      "l6nr066.tif\n",
      "l6nr072.tif\n",
      "l15nr018.tif\n",
      "l7nr066.tif\n",
      "l15nr019.tif\n",
      "l6nr073.tif\n",
      "l7nr067.tif\n",
      "l14nr019.tif\n",
      "l7nr073.tif\n",
      "l6nr067.tif\n",
      "l15nr025.tif\n",
      "l14nr031.tif\n",
      "l14nr025.tif\n",
      "l15nr031.tif\n",
      "l5nr008.tif\n",
      "l4nr008.tif\n",
      "l4nr020.tif\n",
      "l5nr034.tif\n",
      "l5nr020.tif\n",
      "l4nr034.tif\n",
      "l12nr054.tif\n",
      "l13nr040.tif\n",
      "l13nr054.tif\n",
      "l12nr040.tif\n",
      "l1nr002.tif\n",
      "l12nr068.tif\n",
      "l13nr068.tif\n",
      "l1nr016.tif\n",
      "l3nr051.tif\n",
      "l2nr045.tif\n",
      "l2nr051.tif\n",
      "l3nr045.tif\n",
      "l10nr007.tif\n",
      "l11nr013.tif\n",
      "l11nr007.tif\n",
      "l10nr013.tif\n",
      "l9nr046.tif\n",
      "l8nr052.tif\n",
      "l8nr046.tif\n",
      "l9nr052.tif\n",
      "l8nr050.tif\n",
      "l9nr044.tif\n",
      "l9nr050.tif\n",
      "l8nr044.tif\n",
      "l11nr011.tif\n",
      "l10nr005.tif\n",
      "l10nr011.tif\n",
      "l11nr005.tif\n",
      "l2nr047.tif\n",
      "l10nr039.tif\n",
      "l3nr053.tif\n",
      "l3nr047.tif\n",
      "l11nr039.tif\n",
      "l2nr053.tif\n",
      "l1nr014.tif\n",
      "l13nr042.tif\n",
      "l12nr056.tif\n",
      "l12nr042.tif\n",
      "l1nr028.tif\n",
      "l13nr056.tif\n",
      "l5nr036.tif\n",
      "l4nr022.tif\n",
      "l4nr036.tif\n",
      "l5nr022.tif\n",
      "l7nr059.tif\n",
      "l14nr033.tif\n",
      "l15nr027.tif\n",
      "l6nr059.tif\n",
      "l15nr033.tif\n",
      "l14nr027.tif\n",
      "l7nr065.tif\n",
      "l6nr071.tif\n",
      "l6nr065.tif\n",
      "l7nr071.tif\n",
      "l6nr064.tif\n",
      "l7nr070.tif\n",
      "l7nr064.tif\n",
      "l6nr070.tif\n",
      "l15nr032.tif\n",
      "l6nr058.tif\n",
      "l14nr026.tif\n",
      "l14nr032.tif\n",
      "l7nr058.tif\n",
      "l15nr026.tif\n",
      "l4nr037.tif\n",
      "l5nr023.tif\n",
      "l5nr037.tif\n",
      "l4nr023.tif\n",
      "l1nr029.tif\n",
      "l12nr043.tif\n",
      "l13nr057.tif\n",
      "l13nr043.tif\n",
      "l12nr057.tif\n",
      "l1nr015.tif\n",
      "l1nr001.tif\n",
      "l3nr046.tif\n",
      "l2nr052.tif\n",
      "l11nr038.tif\n",
      "l2nr046.tif\n",
      "l3nr052.tif\n",
      "l10nr038.tif\n",
      "l10nr010.tif\n",
      "l11nr004.tif\n",
      "l11nr010.tif\n",
      "l10nr004.tif\n",
      "l9nr051.tif\n",
      "l8nr045.tif\n",
      "l8nr051.tif\n",
      "l9nr045.tif\n",
      "l9nr041.tif\n",
      "l8nr055.tif\n",
      "l8nr041.tif\n",
      "l9nr055.tif\n",
      "l8nr069.tif\n",
      "l9nr069.tif\n",
      "l3nr056.tif\n",
      "l11nr028.tif\n",
      "l2nr042.tif\n",
      "l2nr056.tif\n",
      "l10nr028.tif\n",
      "l3nr042.tif\n",
      "l11nr014.tif\n",
      "l10nr014.tif\n",
      "l12nr053.tif\n",
      "l1nr039.tif\n",
      "l13nr047.tif\n",
      "l13nr053.tif\n",
      "l12nr047.tif\n",
      "l1nr005.tif\n",
      "l1nr011.tif\n",
      "l4nr027.tif\n",
      "l5nr033.tif\n",
      "l5nr027.tif\n",
      "l4nr033.tif\n",
      "l6nr074.tif\n",
      "l7nr060.tif\n",
      "l7nr074.tif\n",
      "l6nr060.tif\n",
      "l6nr048.tif\n",
      "l15nr022.tif\n",
      "l14nr036.tif\n",
      "l7nr048.tif\n",
      "l14nr022.tif\n",
      "l15nr036.tif\n",
      "l14nr023.tif\n",
      "l7nr049.tif\n",
      "l15nr037.tif\n",
      "l15nr023.tif\n",
      "l6nr049.tif\n",
      "l14nr037.tif\n",
      "l7nr075.tif\n",
      "l6nr061.tif\n",
      "l6nr075.tif\n",
      "l7nr061.tif\n",
      "l5nr026.tif\n",
      "l4nr032.tif\n",
      "l4nr026.tif\n",
      "l5nr032.tif\n",
      "l1nr010.tif\n",
      "l1nr004.tif\n",
      "l13nr052.tif\n",
      "l12nr046.tif\n",
      "l1nr038.tif\n",
      "l12nr052.tif\n",
      "l13nr046.tif\n",
      "l11nr001.tif\n",
      "l10nr015.tif\n",
      "l10nr001.tif\n",
      "l11nr015.tif\n",
      "l2nr057.tif\n",
      "l3nr043.tif\n",
      "l10nr029.tif\n",
      "l3nr057.tif\n",
      "l2nr043.tif\n",
      "l11nr029.tif\n",
      "l9nr068.tif\n",
      "l8nr068.tif\n",
      "l8nr040.tif\n",
      "l9nr054.tif\n",
      "l9nr040.tif\n",
      "l8nr054.tif\n",
      "l9nr056.tif\n",
      "l8nr042.tif\n",
      "l8nr056.tif\n",
      "l9nr042.tif\n",
      "l3nr041.tif\n",
      "l2nr055.tif\n",
      "l2nr041.tif\n",
      "l3nr055.tif\n",
      "l10nr017.tif\n",
      "l11nr003.tif\n",
      "l2nr069.tif\n",
      "l11nr017.tif\n",
      "l10nr003.tif\n",
      "l3nr069.tif\n",
      "l12nr044.tif\n",
      "l13nr050.tif\n",
      "l13nr044.tif\n",
      "l12nr050.tif\n",
      "l1nr012.tif\n",
      "l1nr006.tif\n",
      "l5nr018.tif\n",
      "l4nr018.tif\n",
      "l4nr030.tif\n",
      "l5nr024.tif\n",
      "l5nr030.tif\n",
      "l4nr024.tif\n",
      "l6nr063.tif\n",
      "l15nr009.tif\n",
      "l7nr063.tif\n",
      "l14nr009.tif\n",
      "l15nr035.tif\n",
      "l14nr021.tif\n",
      "l14nr035.tif\n",
      "l15nr021.tif\n",
      "l14nr034.tif\n",
      "l15nr020.tif\n",
      "l15nr034.tif\n",
      "l14nr020.tif\n",
      "l14nr008.tif\n",
      "l7nr062.tif\n",
      "l15nr008.tif\n",
      "l6nr062.tif\n",
      "l5nr031.tif\n",
      "l4nr025.tif\n",
      "l4nr031.tif\n",
      "l5nr025.tif\n",
      "l4nr019.tif\n",
      "l5nr019.tif\n",
      "l1nr007.tif\n",
      "l1nr013.tif\n",
      "l13nr045.tif\n",
      "l12nr051.tif\n",
      "l12nr045.tif\n",
      "l13nr051.tif\n",
      "l11nr016.tif\n",
      "l3nr068.tif\n",
      "l10nr002.tif\n",
      "l10nr016.tif\n",
      "l2nr068.tif\n",
      "l11nr002.tif\n",
      "l2nr040.tif\n",
      "l3nr054.tif\n",
      "l3nr040.tif\n",
      "l2nr054.tif\n",
      "l8nr057.tif\n",
      "l9nr043.tif\n",
      "l9nr057.tif\n",
      "l8nr043.tif\n",
      "l8nr018.tif\n",
      "l9nr018.tif\n",
      "l9nr030.tif\n",
      "l8nr024.tif\n",
      "l8nr030.tif\n",
      "l9nr024.tif\n",
      "l10nr071.tif\n",
      "l11nr065.tif\n",
      "l11nr071.tif\n",
      "l10nr065.tif\n",
      "l3nr027.tif\n",
      "l11nr059.tif\n",
      "l2nr033.tif\n",
      "l2nr027.tif\n",
      "l10nr059.tif\n",
      "l3nr033.tif\n",
      "l1nr074.tif\n",
      "l1nr060.tif\n",
      "l12nr022.tif\n",
      "l1nr048.tif\n",
      "l13nr036.tif\n",
      "l13nr022.tif\n",
      "l12nr036.tif\n",
      "l4nr056.tif\n",
      "l5nr042.tif\n",
      "l5nr056.tif\n",
      "l4nr042.tif\n",
      "l6nr039.tif\n",
      "l15nr053.tif\n",
      "l14nr047.tif\n",
      "l7nr039.tif\n",
      "l14nr053.tif\n",
      "l15nr047.tif\n",
      "l6nr005.tif\n",
      "l7nr011.tif\n",
      "l7nr005.tif\n",
      "l6nr011.tif\n",
      "l7nr004.tif\n",
      "l6nr010.tif\n",
      "l6nr004.tif\n",
      "l7nr010.tif\n",
      "l14nr052.tif\n",
      "l7nr038.tif\n",
      "l15nr046.tif\n",
      "l15nr052.tif\n",
      "l6nr038.tif\n",
      "l14nr046.tif\n",
      "l5nr057.tif\n",
      "l4nr043.tif\n",
      "l4nr057.tif\n",
      "l5nr043.tif\n",
      "l13nr023.tif\n",
      "l12nr037.tif\n",
      "l1nr049.tif\n",
      "l12nr023.tif\n",
      "l13nr037.tif\n",
      "l1nr061.tif\n",
      "l1nr075.tif\n",
      "l2nr026.tif\n",
      "l3nr032.tif\n",
      "l10nr058.tif\n",
      "l3nr026.tif\n",
      "l2nr032.tif\n",
      "l11nr058.tif\n",
      "l11nr070.tif\n",
      "l10nr064.tif\n",
      "l10nr070.tif\n",
      "l11nr064.tif\n",
      "l8nr031.tif\n",
      "l9nr025.tif\n",
      "l9nr031.tif\n",
      "l8nr025.tif\n",
      "l9nr019.tif\n",
      "l8nr019.tif\n",
      "l9nr027.tif\n",
      "l8nr033.tif\n",
      "l8nr027.tif\n",
      "l9nr033.tif\n",
      "l10nr066.tif\n",
      "l11nr072.tif\n",
      "l2nr018.tif\n",
      "l11nr066.tif\n",
      "l10nr072.tif\n",
      "l3nr018.tif\n",
      "l3nr030.tif\n",
      "l2nr024.tif\n",
      "l2nr030.tif\n",
      "l3nr024.tif\n",
      "l12nr009.tif\n",
      "l1nr063.tif\n",
      "l13nr009.tif\n",
      "l12nr035.tif\n",
      "l13nr021.tif\n",
      "l13nr035.tif\n",
      "l12nr021.tif\n",
      "l4nr041.tif\n",
      "l5nr055.tif\n",
      "l5nr041.tif\n",
      "l4nr055.tif\n",
      "l5nr069.tif\n",
      "l4nr069.tif\n",
      "l15nr044.tif\n",
      "l14nr050.tif\n",
      "l14nr044.tif\n",
      "l15nr050.tif\n",
      "l6nr012.tif\n",
      "l7nr006.tif\n",
      "l7nr012.tif\n",
      "l6nr006.tif\n",
      "l7nr013.tif\n",
      "l6nr007.tif\n",
      "l6nr013.tif\n",
      "l7nr007.tif\n",
      "l14nr045.tif\n",
      "l15nr051.tif\n",
      "l15nr045.tif\n",
      "l14nr051.tif\n",
      "l4nr068.tif\n",
      "l5nr068.tif\n",
      "l5nr040.tif\n",
      "l4nr054.tif\n",
      "l4nr040.tif\n",
      "l5nr054.tif\n",
      "l13nr034.tif\n",
      "l12nr020.tif\n",
      "l12nr034.tif\n",
      "l13nr020.tif\n",
      "l13nr008.tif\n",
      "l1nr062.tif\n",
      "l12nr008.tif\n",
      "l2nr031.tif\n",
      "l3nr025.tif\n",
      "l3nr031.tif\n",
      "l2nr025.tif\n",
      "l11nr067.tif\n",
      "l3nr019.tif\n",
      "l10nr073.tif\n",
      "l10nr067.tif\n",
      "l2nr019.tif\n",
      "l11nr073.tif\n",
      "l8nr026.tif\n",
      "l9nr032.tif\n",
      "l9nr026.tif\n",
      "l8nr032.tif\n",
      "l8nr036.tif\n",
      "l9nr022.tif\n",
      "l9nr036.tif\n",
      "l8nr022.tif\n",
      "l2nr021.tif\n",
      "l3nr035.tif\n",
      "l3nr021.tif\n",
      "l2nr035.tif\n",
      "l10nr063.tif\n",
      "l3nr009.tif\n",
      "l11nr063.tif\n",
      "l2nr009.tif\n",
      "l13nr024.tif\n",
      "l12nr030.tif\n",
      "l12nr024.tif\n",
      "l13nr030.tif\n",
      "l13nr018.tif\n",
      "l1nr066.tif\n",
      "l12nr018.tif\n",
      "l1nr072.tif\n",
      "l5nr050.tif\n",
      "l4nr044.tif\n",
      "l4nr050.tif\n",
      "l5nr044.tif\n",
      "l7nr003.tif\n",
      "l14nr069.tif\n",
      "l6nr017.tif\n",
      "l6nr003.tif\n",
      "l15nr069.tif\n",
      "l7nr017.tif\n",
      "l14nr055.tif\n",
      "l15nr041.tif\n",
      "l15nr055.tif\n",
      "l14nr041.tif\n",
      "l15nr054.tif\n",
      "l14nr040.tif\n",
      "l14nr054.tif\n",
      "l15nr040.tif\n",
      "l15nr068.tif\n",
      "l6nr002.tif\n",
      "l7nr016.tif\n",
      "l14nr068.tif\n",
      "l7nr002.tif\n",
      "l6nr016.tif\n",
      "l4nr051.tif\n",
      "l5nr045.tif\n",
      "l5nr051.tif\n",
      "l4nr045.tif\n",
      "l1nr073.tif\n",
      "l12nr019.tif\n",
      "l13nr019.tif\n",
      "l1nr067.tif\n",
      "l12nr025.tif\n",
      "l13nr031.tif\n",
      "l13nr025.tif\n",
      "l12nr031.tif\n",
      "l2nr008.tif\n",
      "l11nr062.tif\n",
      "l3nr008.tif\n",
      "l10nr062.tif\n",
      "l3nr020.tif\n",
      "l2nr034.tif\n",
      "l2nr020.tif\n",
      "l3nr034.tif\n",
      "l9nr037.tif\n",
      "l8nr023.tif\n",
      "l8nr037.tif\n",
      "l9nr023.tif\n",
      "l8nr021.tif\n",
      "l9nr035.tif\n",
      "l9nr021.tif\n",
      "l8nr035.tif\n",
      "l9nr009.tif\n",
      "l8nr009.tif\n",
      "l2nr036.tif\n",
      "l10nr048.tif\n",
      "l3nr022.tif\n",
      "l3nr036.tif\n",
      "l11nr048.tif\n",
      "l2nr022.tif\n",
      "l11nr060.tif\n",
      "l10nr074.tif\n",
      "l10nr060.tif\n",
      "l11nr074.tif\n",
      "l13nr033.tif\n",
      "l12nr027.tif\n",
      "l12nr033.tif\n",
      "l1nr059.tif\n",
      "l13nr027.tif\n",
      "l1nr071.tif\n",
      "l1nr065.tif\n",
      "l5nr047.tif\n",
      "l4nr053.tif\n",
      "l4nr047.tif\n",
      "l5nr053.tif\n",
      "l7nr014.tif\n",
      "l6nr014.tif\n",
      "l7nr028.tif\n",
      "l14nr042.tif\n",
      "l15nr056.tif\n",
      "l6nr028.tif\n",
      "l15nr042.tif\n",
      "l14nr056.tif\n",
      "l15nr043.tif\n",
      "l6nr029.tif\n",
      "l14nr057.tif\n",
      "l14nr043.tif\n",
      "l7nr029.tif\n",
      "l15nr057.tif\n",
      "l6nr015.tif\n",
      "l7nr001.tif\n",
      "l7nr015.tif\n",
      "l6nr001.tif\n",
      "l4nr046.tif\n",
      "l5nr052.tif\n",
      "l5nr046.tif\n",
      "l4nr052.tif\n",
      "l1nr064.tif\n",
      "l1nr070.tif\n",
      "l1nr058.tif\n",
      "l12nr032.tif\n",
      "l13nr026.tif\n",
      "l13nr032.tif\n",
      "l12nr026.tif\n",
      "l10nr061.tif\n",
      "l11nr075.tif\n",
      "l11nr061.tif\n",
      "l10nr075.tif\n",
      "l3nr037.tif\n",
      "l2nr023.tif\n",
      "l11nr049.tif\n",
      "l2nr037.tif\n",
      "l3nr023.tif\n",
      "l10nr049.tif\n",
      "l8nr008.tif\n",
      "l9nr008.tif\n",
      "l9nr020.tif\n",
      "l8nr034.tif\n",
      "l8nr020.tif\n",
      "l9nr034.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenli/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "data_dir = \"/Users/stevenli/SigAida/data/images/\" #change to data directory\n",
    "\n",
    "# gets the label based on the number\n",
    "def getLabel(s):\n",
    "    labels = {1 : 'Ulmus carpinifolia', \n",
    "                2 : 'Acer', \n",
    "                3 : 'Salix aurita', \n",
    "                4 : 'Quercus', \n",
    "                5 : 'Alnus incan', \n",
    "                6 : 'Betula pubescens', \n",
    "                7 : 'Salix alba \\'Sericea\\'', \n",
    "                8 : 'Populus tremula', \n",
    "                9 : 'Ulmus glabra', \n",
    "                10 : 'Sorbus aucuparia', \n",
    "                11 : 'Salix sinerea', \n",
    "                12 : 'Populus', \n",
    "                13 : 'Tilia', \n",
    "                14 : 'Sorbus intermedia', \n",
    "                15 : 'Fagus silvatica'}\n",
    "    return labels.get(int(s))\n",
    "\n",
    "im = [] # images\n",
    "la = [] # labels not in use, because you can't feed strings to gpu, need to feed tensors\n",
    "ohe = [] #s toring int values for one hot encodings\n",
    "joined = []\n",
    "\n",
    "for f in os.listdir(data_dir):\n",
    "    im.append(data_dir + f)\n",
    "    print(f)\n",
    "    val = int(f[:-9].replace('l','')) # removes last 9 letters replaces l with blank then gets label\n",
    "    la.append(getLabel(val)) \n",
    "    ohe.append(val)\n",
    "\n",
    "#one hot encoding (changing 1-15 to tensors for gpu)\n",
    "encoded = OneHotEncoder(categories = [[x for x in range(1, 16)]], sparse = False).fit_transform(np.array(ohe).reshape((len(ohe),1)))\n",
    "# print(encoded)\n",
    "\n",
    "#joining image and label for organization and if we wanna shuffle\n",
    "for image, label in zip(im, encoded):\n",
    "    joined.append([image, label])\n",
    "\n",
    "# print(la)\n",
    "# print(im)\n",
    "# print(joined[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e2c21ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125\n",
      "900\n",
      "112\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "# some preprocessing, randomize data, split data, load into dataloaders\n",
    "import random\n",
    "\n",
    "# splits (train, val, test) test currently not in use\n",
    "split_ratio=(0.8, 0.1, 0.1)\n",
    "\n",
    "def create_splits(data, split_ratio):\n",
    "    random.shuffle(data)\n",
    "    train = [data[i] for i in range(0, round(.8 * len(data)))]\n",
    "    val = [data[i] for i in range(round(.8 * len(data)), round(.9 * len(data)))]\n",
    "    test = [data[i] for i in range(round(.9 * len(data)), len(data))]\n",
    "    return train, val, test\n",
    "\n",
    "train, val, test = create_splits(joined, split_ratio)\n",
    "\n",
    "print(len(joined))\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f5b89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#prevents some truncated bs\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# makes a custom dataset based on pytorch dataset class\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, data_dir, arr, transform = None, augment = None):\n",
    "        # initialize some valuess\n",
    "        self.data_dir = data_dir\n",
    "        self.data = [x[0] for x in arr]\n",
    "        self.augments = augment\n",
    "        #transform to normalize/resize all images\n",
    "        self.transform = transform\n",
    "\n",
    "        #apply augment and transforms\n",
    "        self.arr = self.augment(arr, self.augments, self.transform, data_dir) \n",
    "\n",
    "    # data augmentation helper function\n",
    "    def augment(self, arr, augments, transform, data_dir):\n",
    "        a = []\n",
    "        for (f, l) in arr:\n",
    "            img = transform(Image.open(f))\n",
    "            a.append((img, l))\n",
    "            \n",
    "            n = 5 # of multiples\n",
    "            if augments:\n",
    "                for i in range(n):\n",
    "                    a.append((augments(img), l))\n",
    "        return a\n",
    "            \n",
    "\n",
    "\n",
    "        # ims = [Image.open(i[0]) for i in arr]\n",
    "        # # print(len(ims))\n",
    "        # lbs = [i[1] for i in arr]\n",
    "\n",
    "        # #transform and write out into data_dir then concatenate to arr with original label\n",
    "        # n = 3 #how many x do you want to make the data\n",
    "        # counter = 1\n",
    "        # for im, lb in zip(ims, lbs):\n",
    "        #     for j in range(n):\n",
    "        #         temp = transforms(im)\n",
    "        #         filename = data_dir + str(counter) + \"image\" + str(j) + \"version\" + str(np.where(lb == 1)[0][0] + 1) + \"label.tif\"\n",
    "        #         temp.save(filename)\n",
    "        #         temp.close()\n",
    "\n",
    "        #         arr.append((filename, lb))\n",
    "        #     counter += 1\n",
    "        # return arr\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.arr)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.arr[idx][0]\n",
    "        s = self.arr[idx][1]\n",
    "        #returns a tuple of the transformed image and the label (one-hot encoding)\n",
    "        return (img, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7556e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "#defining our transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # change to what data should be\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "#list of augments, rotation, affine (translation)\n",
    "augments = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                              transforms.RandomRotation(8,fill=0),\n",
    "                              transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), fill=0)]) \n",
    "\n",
    "#making dataset and dataloader\n",
    "train_dataset = PlantDataset(data_dir, train, transform = transform, augment=augments) #probably need to make this bigger\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c131f6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe8160eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#chcek devices change to whatever you're using\n",
    "\n",
    "#mps = (m1/m2 mac) gpu\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "#set device\n",
    "device = torch.device('mps')\n",
    "\n",
    "from tqdm import tqdm # progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35efd478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5400/5400 [02:33<00:00, 35.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basline before training\n",
      "0.144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#testing accuracy on test dataset\n",
    "total_correct = 0\n",
    "total_instances = 0\n",
    "\n",
    "pretrained_model.eval()\n",
    "# iterating through batches without updating gradients\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(train_loader):\n",
    "      pretrained_model.to(device)\n",
    "      images = images.to(device)\n",
    "      # labels = labels.float().to(device) # don't need this cuz we not training no more\n",
    "\n",
    "      # making classifications and deriving indices of maximum value via argmax (which gives the max value i the tensor)\n",
    "      solution_tensor = pretrained_model(images)\n",
    "      classifications = torch.argmax(solution_tensor, dim = 1).item()\n",
    "\n",
    "      #undoing one-hot encoding to get label value as a number\n",
    "      label = np.where(labels.numpy() == 1)[1]\n",
    "\n",
    "      correct_predictions = int(classifications==label)\n",
    "\n",
    "      #  incrementing counters\n",
    "      total_correct+=correct_predictions\n",
    "      total_instances+=len(images)\n",
    "\n",
    "#print accuracy\n",
    "print(\"basline before training\")\n",
    "print(round(total_correct/total_instances, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "971f44ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:30<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.2672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 0.1454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 0.0500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:23<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Loss: 0.0488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 0.0105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Loss: 0.0261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Loss: 0.0235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Loss: 0.0274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Loss: 0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.0094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Loss: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Loss: 0.0174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Loss: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Loss: 0.0366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Loss: 0.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Loss: 0.0098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Loss: 0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50], Loss: 0.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:24<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50], Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/50], Loss: 0.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/50], Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/50], Loss: 0.0102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50], Loss: 0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/50], Loss: 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/50], Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/50], Loss: 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/50], Loss: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Loss: 0.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:23<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/50], Loss: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/50], Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/50], Loss: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50], Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50], Loss: 0.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50], Loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/50], Loss: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/50], Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/50], Loss: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50], Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/50], Loss: 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/50], Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/50], Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/50], Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/50], Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/50], Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/50], Loss: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/50], Loss: 0.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:21<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/50], Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:22<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "pretrained_model.train()\n",
    "num_epochs = 50\n",
    "\n",
    "#store loss\n",
    "loss_history = []\n",
    "\n",
    "#change batch size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data, label in tqdm(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        pretrained_model.to(device)\n",
    "        images = data.to(device)\n",
    "        labels = label.float().to(device) #.float() for some float64, float32 conversion thing, don't completely understand\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = pretrained_model(images) #currently image tensor don't match fc layer if you use non pretrained model\n",
    "        loss = criterion(outputs, labels) #calculate loss\n",
    "\n",
    "        # Backward and optimize don't really understand this stuff\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print some statistics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    loss_history.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a00b5a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [i for i in range(1, len(loss_history) + 1)]\n",
    "    \n",
    "# plotting the points   \n",
    "plt.plot(x, loss_history)  \n",
    "    \n",
    "# naming the x axis  \n",
    "plt.xlabel('epoch number')  \n",
    "# naming the y axis  \n",
    "plt.ylabel('loss')  \n",
    "    \n",
    "# giving a title to my graph  \n",
    "plt.title('loss in training')  \n",
    "    \n",
    "# function to show the plot  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4270f96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5400/5400 [02:26<00:00, 36.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#testing accuracy on test dataset\n",
    "total_correct = 0\n",
    "total_instances = 0\n",
    "\n",
    "#chang back batch size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "pretrained_model.eval()\n",
    "# iterating through batches without updating gradients\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(train_loader):\n",
    "      images = images.to(device)\n",
    "      # labels = labels.float().to(device) # don't need this cuz we not training no more\n",
    "\n",
    "      # making classifications and deriving indices of maximum value via argmax (which gives the max value i the tensor)\n",
    "      solution_tensor = pretrained_model(images)\n",
    "      classifications = torch.argmax(solution_tensor, dim = 1).item()\n",
    "\n",
    "      #undoing one-hot encoding to get label value as a number\n",
    "      label = np.where(labels.numpy() == 1)[1]\n",
    "\n",
    "      correct_predictions = int(classifications==label)\n",
    "\n",
    "      #  incrementing counters\n",
    "      total_correct+=correct_predictions\n",
    "      total_instances+=len(images)\n",
    "\n",
    "#print accuracy\n",
    "print(\"after training\")\n",
    "print(round(total_correct/total_instances, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b99bea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading validation dataset\n",
    "val_dataset = PlantDataset(data_dir, val, transform = transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57f9f411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:04<00:00, 24.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_instances = 0\n",
    "\n",
    "pretrained_model.eval()\n",
    "# iterating through batches without updating gradients\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader):\n",
    "      images = images.to(device)\n",
    "      # labels = labels.float().to(device) # don't need this cuz we not training no more\n",
    "\n",
    "      # making classifications and deriving indices of maximum value via argmax (which gives the max value i the tensor)\n",
    "      solution_tensor = pretrained_model(images)\n",
    "      classifications = torch.argmax(solution_tensor, dim = 1).item()\n",
    "\n",
    "      #undoing one-hot encoding to get label value as a number\n",
    "      label = np.where(labels.numpy() == 1)[1]\n",
    "\n",
    "      correct_predictions = int(classifications==label)\n",
    "\n",
    "      #  incrementing counters\n",
    "      total_correct+=correct_predictions\n",
    "      total_instances+=len(images)\n",
    "\n",
    "#print accuracy\n",
    "print(round(total_correct/total_instances, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2a7c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading test dataset\n",
    "test_dataset = PlantDataset(data_dir, test, transform = transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "619d11ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:03<00:00, 31.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_instances = 0\n",
    "\n",
    "pictures = []\n",
    "model_guesses = []\n",
    "correct_answers = []\n",
    "\n",
    "# iterating through batches without updating gradients\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "      pictures.append(images)\n",
    "      images = images.to(device)\n",
    "\n",
    "      # making classifications and deriving indices of maximum value via argmax (which gives the max value i the tensor)\n",
    "      solution_tensor = pretrained_model(images)\n",
    "      classifications = torch.argmax(solution_tensor, dim = 1).item()\n",
    "      model_guesses.append(getLabel(classifications + 1)) # + 1 to convert from index to dict key\n",
    "\n",
    "      #undoing one-hot encoding to get label value as a number\n",
    "      label = np.where(labels.numpy() == 1)[1]\n",
    "      correct_answers.append(getLabel(label + 1)) # + 1 to convert from index to dict key\n",
    "\n",
    "      correct_predictions = int(classifications==label)\n",
    "\n",
    "      #  incrementing counters\n",
    "      total_correct+=correct_predictions\n",
    "      total_instances+=len(images)\n",
    "\n",
    "#print accuracy\n",
    "print(round(total_correct/total_instances, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e22a4013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump to pickle\n",
    "import pickle\n",
    "pickle.dump(pretrained_model, open('model3.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
