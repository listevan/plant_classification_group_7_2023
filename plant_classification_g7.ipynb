{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbe5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch and numpy and pretrained model\n",
    "from torchvision import models\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# load pretrained model\n",
    "pretrained_cnn = models.resnet50(weights = 'DEFAULT', progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262de170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes fully connected/classifier layer to new layer for us to train, 15 is the number of solutions or types of leaves\n",
    "pretrained_cnn.fc = torch.nn.Linear(2048, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c430901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze model except fc layer because we don't wanna retrain the pretrained model\n",
    "for param in pretrained_cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in pretrained_cnn.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e32582",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss() #could write this out ourselves\n",
    "# need to find an optimizer or make one for a custom softmax function\n",
    "optimizer = torch.optim.Adam(pretrained_cnn.fc.parameters(), lr=0.001) #need optimize learning rate idk how momentum works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41109c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get Files\n",
    "import sys, os\n",
    "from scripts import ProcessData\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from io import BytesIO\n",
    "\n",
    "data_dir = \"./data/\"\n",
    "\n",
    "def get_urls():\n",
    "    urls = []\n",
    "    for i in range(1, 16):\n",
    "        urls.append(f\"https://www.cvl.isy.liu.se/en/research/datasets/swedish-leaf/leaf{i}.zip\")\n",
    "    return urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "\n",
    "    # download and extract in parallel \n",
    "    print(f\"There are {cpu_count()} CPUs on this machine \") \n",
    "    with Pool(cpu_count()) as p:\n",
    "        download_func = partial(ProcessData.download_zip, file_path = data_dir)\n",
    "        print(p.map(download_func, get_urls()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3530c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# gets the label based on the number\n",
    "def getLabel(s):\n",
    "    labels = {1 : \"Ulmus carpinifolia\", \n",
    "                2 : \"Acer\", \n",
    "                3 : \"Salix aurita\", \n",
    "                4 : \"Quercus\", \n",
    "                5 : \"Alnus incan\", \n",
    "                6 : \"Betula pubescens\", \n",
    "                7 : \"Salix alba 'Sericea'\", \n",
    "                8 : \"Populus tremula\", \n",
    "                9 : \"Ulmus glabra\", \n",
    "                10 : \"Sorbus aucuparia\", \n",
    "                11 : \"Salix sinerea\", \n",
    "                12 : \"Populus\", \n",
    "                13 : \"Tilia\", \n",
    "                14 : \"Sorbus intermedia\", \n",
    "                15 : \"Fagus silvatica\"}\n",
    "    return labels.get(int(s))\n",
    "\n",
    "imgs = [] # images\n",
    "labels = [] # labels not in use, because you can't feed strings to gpu, need to feed tensors\n",
    "ohe = [] # storing int values for one hot encodings\n",
    "labeled_images = []\n",
    "\n",
    "for leaf_folder in os.listdir(data_dir):\n",
    "    for leaf in os.listdir(leaf_folder):\n",
    "        imgs.append(os.path.join(data_dir, leaf_folder, leaf))\n",
    "        val = leaf.split('leaf')[0]   # split into list with just the number, gets first value, the number\n",
    "        labels.append(getLabel(val)) \n",
    "        ohe.append(val)\n",
    "\n",
    "# one hot encoding (changing 1-15 to tensors for gpu)\n",
    "encoded = OneHotEncoder(categories = [[x for x in range(1, 16)]], sparse = False).fit_transform(np.array(ohe).reshape((len(ohe),1)))\n",
    "\n",
    "#joining image and label\n",
    "for image, ohe_label in zip(imgs, encoded):\n",
    "    labeled_images.append([image, ohe_label])\n",
    "\n",
    "# print(la)\n",
    "# print(im)\n",
    "# print(joined[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5b89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dataset\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# defining transform for resnet\n",
    "resNetTransform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # change to what data should be\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "# makes a custom dataset based on pytorch dataset class\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, data_dir, arr, transform = resNetTransform):\n",
    "        # initialize some valuess\n",
    "        self.data_dir = data_dir\n",
    "        self.data = [img for img, _ in arr]\n",
    "        self.arr = arr\n",
    "\n",
    "        #transform to normalize/resize all images\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.arr)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.arr[idx] \n",
    "        img = self.transform(Image.open(img))\n",
    "                \n",
    "        #returns a tuple of the transformed image and the label (one-hot encoding)\n",
    "        return (img, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8160eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chcek devices change to whatever you're using\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "#set device\n",
    "device = torch.device('mps')\n",
    "\n",
    "# from tqdm import tqdm # progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Fold Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "# init stuff\n",
    "k_folds = 5\n",
    "num_epochs = 1\n",
    "dataset = PlantDataset(data_dir, labeled_images)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# For fold results\n",
    "results = {}\n",
    "\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Start print\n",
    "print('--------------------------------')\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=10, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=10, sampler=test_subsampler)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(pretrained_cnn.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, num_epochs):\n",
    "\n",
    "      # Print epoch\n",
    "      print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "      # Set current loss value\n",
    "      current_loss = 0.0\n",
    "\n",
    "      # Iterate over the DataLoader for training data\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # Get inputs\n",
    "        inputs, targets = data\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = pretrained_cnn(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                  (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "            \n",
    "    # Process is complete.\n",
    "    print('Training process has finished. Saving trained model.')\n",
    "\n",
    "    # Print about testing\n",
    "    print('Starting testing')\n",
    "    \n",
    "    # Saving the model\n",
    "    save_path = f'./model-fold-{fold}.pth'\n",
    "    torch.save(pretrained_cnn.state_dict(), save_path)\n",
    "\n",
    "    # Evaluationfor this fold\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "      # Iterate over the test data and generate predictions\n",
    "      for i, data in enumerate(testloader, 0):\n",
    "\n",
    "        # Get inputs\n",
    "        inputs, targets = data\n",
    "\n",
    "        # Generate outputs\n",
    "        outputs = pretrained_cnn(inputs)\n",
    "\n",
    "        # Set total and correct\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "      # Print accuracy\n",
    "      print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
    "      print('--------------------------------')\n",
    "      results[fold] = 100.0 * (correct / total)\n",
    "    \n",
    "# Print fold results\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "sum = 0.0\n",
    "for key, value in results.items():\n",
    "    print(f'Fold {key}: {value} %')\n",
    "    sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35efd478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:59<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basline before training\n",
      "0.072\n"
     ]
    }
   ],
   "source": [
    "#testing accuracy on test dataset\n",
    "import tqdm\n",
    "\n",
    "total_correct = 0\n",
    "total_instances = 0\n",
    "\n",
    "pretrained_cnn.eval()\n",
    "# iterating through batches without updating gradients\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(train_loader):\n",
    "      pretrained_cnn.to(device)\n",
    "      images = images.to(device)\n",
    "      # labels = labels.float().to(device) # don't need this cuz we not training no more\n",
    "\n",
    "      # making classifications and deriving indices of maximum value via argmax (which gives the max value i the tensor)\n",
    "      solution_tensor = pretrained_cnn(images)\n",
    "      classifications = torch.argmax(solution_tensor, dim = 1).item()\n",
    "\n",
    "      #undoing one-hot encoding to get label value as a number\n",
    "      label = np.where(labels.numpy() == 1)[1]\n",
    "\n",
    "      correct_predictions = int(classifications==label)\n",
    "\n",
    "      #  incrementing counters\n",
    "      total_correct+=correct_predictions\n",
    "      total_instances+=len(images)\n",
    "\n",
    "#print accuracy\n",
    "print(\"basline before training\")\n",
    "print(round(total_correct/total_instances, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4270f96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:49<00:00, 18.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#testing accuracy on test dataset\n",
    "total_correct = 0\n",
    "total_instances = 0\n",
    "\n",
    "#chang back batch size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "pretrained_cnn.eval()\n",
    "# iterating through batches without updating gradients\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(train_loader):\n",
    "      images = images.to(device)\n",
    "      # labels = labels.float().to(device) # don't need this cuz we not training no more\n",
    "\n",
    "      # making classifications and deriving indices of maximum value via argmax (which gives the max value i the tensor)\n",
    "      solution_tensor = pretrained_cnn(images)\n",
    "      classifications = torch.argmax(solution_tensor, dim = 1).item()\n",
    "\n",
    "      #undoing one-hot encoding to get label value as a number\n",
    "      label = np.where(labels.numpy() == 1)[1]\n",
    "\n",
    "      correct_predictions = int(classifications==label)\n",
    "\n",
    "      #  incrementing counters\n",
    "      total_correct+=correct_predictions\n",
    "      total_instances+=len(images)\n",
    "\n",
    "#print accuracy\n",
    "print(\"after training\")\n",
    "print(round(total_correct/total_instances, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e22a4013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump to pickle\n",
    "import pickle\n",
    "pickle.dump(pretrained_cnn, open('model.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
